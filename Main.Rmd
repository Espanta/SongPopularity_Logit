---
title: "PopularSong_Logit"
author: "Saeid Abolfazli"
date: "May 9, 2016"
output: pdf_document
---

In this modeling effort, we are trying to build a predictive model to predict whether a singer can
```{r}
file <- file.path("data","songs.csv")
songs <- read.csv(file)
str(songs)
```

**1. Q1** How many observations (songs) are from the year 2010?
Below answer both work.
```{r}
dim(songs[songs$year=='2010',])[1]
table(songs$year)
```


**2. Q2** How many songs does the dataset include for which the artist name is "Michael Jackson"?

```{r}
dim(songs[songs$artistname=='Michael Jackson' ,])[1]
```

**3. Q3** Which of the songs by Michael Jackson made it to the Top 10? Select all that apply.

```{r}
songs[songs$artistname=='Michael Jackson' & songs$Top10 == 1,][2]
```

**4.1 Q4** The variable corresponding to the estimated time signature (timesignature) is discrete, meaning that it only takes integer values (0, 1, 2, 3, . . . ). What are the values of this variable that occur in our dataset? Select all that apply.

```{r}
unique(songs$timesignature)
```


**4.2 Q4**Which timesignature value is the most frequent among songs in our dataset?

```{r}
table(songs$timesignature)
```


**5. Q5** Out of all of the songs in our dataset, the song with the highest tempo is one of the following songs. Which one is it?
```{r}
songs[which.max(songs$tempo),][2]
```

**6. Q6** We wish to predict whether or not a song will make it to the Top 10. To do this, first use the subset function to split the data into a training set "SongsTrain" consisting of all the observations up to and including 2009 song releases, and a testing set "SongsTest", consisting of the 2010 song releases.

How many observations (songs) are in the training set?
```{r}
songsTrain <- subset(songs,songs$year<=2009)
songsTest <- subset(songs,songs$year==2010)
```


**7. Q7** Build a model that predicts whether or not a song will make it to the Top 10 of the Billboard Hot 100 Chart. Since the outcome variable is binary, we will build a logistic regression model. We'll start by using all song attributes as our independent variables, which we'll call Model 1.

We will only use the variables in our dataset that describe the numerical attributes of the song in our logistic regression model. So we won't use the variables "year", "songtitle", "artistname", "songID" or "artistID".


```{r}
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
songsTrain = songsTrain[ , !(names(songsTrain) %in% nonvars) ]

songsTest = songsTest[ , !(names(songsTest) %in% nonvars) ]

Model1 <- glm(Top10~.,data = songsTrain,family = binomial)
summary(Model1)
```

Looking at the summary of your model, what is the value of the Akaike Information Criterion (AIC)?


**8. Q6** Let's now think about the variables in our dataset related to the confidence of the time signature, key and tempo (timesignature_confidence, key_confidence, and tempo_confidence). Our model seems to indicate that these confidence variables are significant (rather than the variables timesignature, key and tempo themselves). What does the model suggest?

** Answer:**  The higher our confidence about time signature, key and tempo, the more likely the song is to be in the Top 10

**9. Q7** In general, if the confidence is low for the time signature, tempo, and key, then the song is more likely to be complex. What does Model 1 suggest in terms of complexity?

**10. Q10** Songs with heavier instrumentation tend to be louder (have higher values in the variable "loudness") and more energetic (have higher values in the variable "energy"). By inspecting the coefficient of the variable "loudness", what does Model 1 suggest?

**Answer:** Mainstream listeners prefer songs with heavy instrumentation 

**11. Q11**
By inspecting the coefficient of the variable "energy", do we draw the same conclusions as above?

**Answer:** No

**12. Q12** What is the correlation between the variables "loudness" and "energy" in the training set?

```{r}
cor(songsTrain$energy,songsTrain$loudness)
```

**13. Q13**
Create Model 2, which is Model 1 without the independent variable "loudness". This can be done with the following command:

```{r}
Model2 = glm(Top10 ~ . - loudness, data=songsTrain, family=binomial)
summary(Model2)
```

Look at the summary of SongsLog2, and inspect the coefficient of the variable "energy". What do you observe?


**Answer:** Model 2 suggests that songs with high energy levels tend to be more popular. This contradicts our observation in Model 1

**14. Q14**
Now, create Model 3, which should be exactly like Model 1, but without the variable "energy".
```{r}
Model3 = glm(Top10 ~ . - energy, data=songsTrain, family=binomial)
summary(Model3)
```

**15. Q15**
Make predictions on the test set using Model 3. What is the accuracy of Model 3 on the test set, using a threshold of 0.45? (Compute the accuracy as a number between 0 and 1.)
```{r}
songsPred <- predict(Model3, type = "response", newdata = songsTest)
table(songsTest$Top10, songsPred >0.45)
acc<-(309 + 19) /(309+40+5+19)
acc
```
**16. Q16**
Let's check if there's any incremental benefit in using Model 3 instead of a baseline model. Given the difficulty of guessing which song is going to be a hit, an easier model would be to pick the most frequent outcome (a song is not a Top 10 hit) for all songs. What would the accuracy of the baseline model be on the test set? (Give your answer as a number between 0 and 1.)

```{r}
(309+5)/(309+40+5+19)
```
**17. Q17**
How many songs does Model 3 correctly predict as Top 10 hits in 2010 (remember that all songs in 2010 went into our test set), using a threshold of 0.45?

**Answer:** 19

**18. Q18**
How many non-hit songs does Model 3 predict will be Top 10 hits (again, looking at the test set), using a threshold of 0.45?

**Answer:** 5

**19. Q19**
What is the sensitivity of Model 3 on the test set, using a threshold of 0.45?

sensitivity <- tp/(tp+fn) <- 19/(19+40)
specificity <- tn/(tn+fp) <- 309/(309+5)

**Answer:** 0.3220339

**20. Q20**
What is the specificity of Model 3 on the test set, using a threshold of 0.45?

**Answer:** 0.9840764

**21. Q21** What conclusions can you make about our model? (Select all that apply.)

* Model 3 favors specificity over sensitivity.
* Model 3 provides conservative predictions, and predicts that a song will make it to the Top 10 very rarely. So while it detects less than half of the Top 10 songs, we can be very confident in the songs that it does predict to be Top 10 hits.
